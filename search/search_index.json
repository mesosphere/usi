{"docs":[{"location":"/paradox.json","text":"","title":""},{"location":"/index.html","text":"","title":"Unified Scheduler Interface"},{"location":"/index.html#unified-scheduler-interface","text":"This repository is currently a work-in-progress.\nHow to run:\n$ gradle :hello-world:run\nHow to test:\n$ gradle test","title":"Unified Scheduler Interface"},{"location":"/getting-started.html","text":"","title":"Getting Started"},{"location":"/getting-started.html#getting-started","text":"","title":"Getting Started"},{"location":"/getting-started.html#project-setup","text":"Given you have added the https://downloads.mesosphere.com/maven Maven repository you can declare the following dependencies\nsbt libraryDependencies ++= Seq(\n  \"com.mesosphere.usi\" %% \"core\" % \"0.1.53\",\n  \"com.mesosphere.usi\" %% \"core-model\" % \"0.1.53\",\n  \"com.mesosphere.usi\" %% \"mesos-client\" % \"0.1.53\",\n  \"com.mesosphere.usi\" %% \"metrics-dropwizard\" % \"0.1.53\"\n) Maven <dependency>\n  <groupId>com.mesosphere.usi</groupId>\n  <artifactId>core_2.12</artifactId>\n  <version>0.1.53</version>\n</dependency>\n<dependency>\n  <groupId>com.mesosphere.usi</groupId>\n  <artifactId>core-model_2.12</artifactId>\n  <version>0.1.53</version>\n</dependency>\n<dependency>\n  <groupId>com.mesosphere.usi</groupId>\n  <artifactId>mesos-client_2.12</artifactId>\n  <version>0.1.53</version>\n</dependency>\n<dependency>\n  <groupId>com.mesosphere.usi</groupId>\n  <artifactId>metrics-dropwizard_2.12</artifactId>\n  <version>0.1.53</version>\n</dependency> Gradle dependencies {\n  compile group: 'com.mesosphere.usi', name: 'core_2.12', version: '0.1.53',\n  compile group: 'com.mesosphere.usi', name: 'core-model_2.12', version: '0.1.53',\n  compile group: 'com.mesosphere.usi', name: 'mesos-client_2.12', version: '0.1.53',\n  compile group: 'com.mesosphere.usi', name: 'metrics-dropwizard_2.12', version: '0.1.53'\n}","title":"Project Setup"},{"location":"/getting-started.html#hello-world","text":"Let’s start out with a simple Hello World program. We are going to connect to Mesos (1), start a task that echos Hello World (2) and then finish (3).","title":"Hello World"},{"location":"/getting-started.html#1-connect-to-mesos","text":"val client: MesosClient = Await.result(MesosClient(clientSettings, frameworkInfo).runWith(Sink.head), 10.seconds)\nval factory = SchedulerFactory(client, podRecordRepository, SchedulerSettings.load(), DummyMetrics)\nval (snapshot, schedulerFlow) =\n  Await.result(factory.newSchedulerFlow(), 10.seconds)\n(client, snapshot, schedulerFlow)\nThe MesosClient(...) factory method returns a future MesosClient. It requires the Mesos FrameworkInfo. In this case we block until Mesos is connected. You should not do so in the real world. Also note, that we are not retrying the connection. See the section on Resilience.\nAfter the connection is established we create a USI scheduler flow. This is an Akka Stream flow that receives SchedulerCommands and outputs StateEvents.","title":"1 Connect to Mesos"},{"location":"/getting-started.html#2-start-a-task","text":"Once the client is connected and scheduler flow created we run a source via the flow.\nval completed: Future[Done] = Source.maybe\n  .prepend(Source.single(generateLaunchCommand))\n  // Here our initial snapshot is going to the scheduler flow\n  .via(schedulerFlow)\nThe source is just a single LaunchPod command:\ndef generateLaunchCommand: LaunchPod = {\n  // Lets construct the initial specs snapshot which will contain our hello-world PodSpec. For that we generate\n  // - a unique PodId\n  // - a RunSpec with minimal resource requirements and hello-world shell command\n  // - a snapshot containing our PodSpec\n  val podId = PodId(s\"hello-world.${UUID.randomUUID()}\")\n  val runSpec = SimpleRunTemplateFactory(\n    resourceRequirements = List(ScalarRequirement.cpus(0.1), ScalarRequirement.memory(32)),\n    shellCommand = \"\"\"echo \"Hello, world\" && sleep 123456789\"\"\",\n    role = \"test\"\n  )\n  commands.LaunchPod(podId, runSpec = runSpec)\n}\nWe do not have to handle offer matching since USI is taking this work from our shoulder. We just care about what we want to launch.","title":"2 Start a Task"},{"location":"/getting-started.html#3-finish","text":"The last step handles the events emitted by USI.\n// Main state event handler. We log happy events and exit if something goes wrong\n  case PodStatusUpdatedEvent(id, Some(PodStatus(_, taskStatuses))) =>\n    import TaskState._\n    def activeTask(status: TaskStatus) = Seq(TASK_STAGING, TASK_STARTING, TASK_RUNNING).contains(status.getState)\n\n    // We're only interested in the happy task statuses for our pod\n    val failedTasks = taskStatuses.filterNot { case (_, status) => activeTask(status) }\n    assert(failedTasks.isEmpty, s\"Found failed tasks: $failedTasks, can't handle them now so will terminate\")\n\n    logger.info(s\"Task status updates for podId: $id: ${taskStatuses}\")\n\n  case e =>\n    logger.warn(s\"Unhandled event: $e\") // we ignore everything else for now\n}\n.toMat(Sink.ignore)(Keep.right)\n.run()\nIn this simple case we just inform about the state of our task and finish. USI has launch at most once guarantees. That means it is upt to you the framework author to re-launch finished and failed tasks.","title":"3 Finish"},{"location":"/contributing/index.html","text":"","title":"Contributors Guide"},{"location":"/contributing/index.html#contributors-guide","text":"Make contributions with GitHub pull requests.","title":"Contributors Guide"},{"location":"/contributing/index.html#bugs-feature-requests","text":"Think you’ve found a bug? Want to see a new feature? Please open a case in our issue management tool, JIRA: * Login to the DC/OS USI Public JIRA. You will need a Github or Google account to use this service. * Navigate to the DC/OS OSS USI project. * Click Create Issue - Please provide as much information as possible about the issue type and how to reproduce it. * Github Issues for this project have been disabled. * Bug reports in JIRA for the Mesosphere USI project are public.\nIf you would like to propose a new feature or a significant change, first open an issue (instructions above) to discuss it. Project shepherds will provide feedback on the proposed change in terms of feasibility, design, and implementation guidance. While all proposals are welcome, those that conflict with our roadmap or design principles may not be accepted.","title":"Bugs / Feature Requests"},{"location":"/contributing/index.html#creating-pull-requests","text":"Create pull requests against the master branch. Be sure to include unit tests and integration tests, as well as updates to the documentation, default scheduler, and reference framework if necessary.\nSee the (testing documentation)[TESTING.md] for instructions on running static code checks and system integration tests.\nSimple pull requests (e.g., to fix documentation or comments) are welcome without an issue, but any substantial changes require an issue and discussion ahead of time.","title":"Creating Pull Requests"},{"location":"/contributing/index.html#merge-approvals","text":"Pull requests are reviewed and tested prior to merging. Often shepherds will ask contributors to make changes based on our style guidelines, design principles, or project goals. An initial response is typically provided in less than one week.","title":"Merge Approvals"},{"location":"/contributing/index.html#communication","text":"Most communication is done primarily over Slack. Join us via http://chat.dcos.io in the #usi and #usi-dev channels, for users and developers respectively.\nTo build a healthy, respectful community, we follow Github’s community guidelines.","title":"Communication"},{"location":"/contributing/index.html#licensing","text":"Mesosphere USI is licensed under the Apache License, Version 2.0. Any code you submit must be under that license.\nFor significant changes, we may ask you to sign a Contributor License Agreement.","title":"Licensing"},{"location":"/contributing/code-culture.html","text":"","title":"Code Culture"},{"location":"/contributing/code-culture.html#code-culture","text":"","title":"Code Culture"},{"location":"/contributing/code-culture.html#overview","text":"The code culture is a set of defaults ascribed to by the Orchestration developer team. Since writing software is an optimization problem, we guide our design decisions with defaults, not rules. Deviations from our defaults require justification.\nOur defaults are:\nCode is legible on GitHub Immutability over mutability No smelly code Fail loud and proud Let it crash Don’t panic Structured logging","title":"Overview"},{"location":"/contributing/code-culture.html#code-is-legible-on-github","text":"This default speaks to the desire to write code that can be understood without needing an IDE. USI is written in Scala, so our defaults are specific as such.","title":"Code is Legible on GitHub"},{"location":"/contributing/code-culture.html#on-implicits","text":"Implicits are an important feature in Scala and are critical for the implementation of type-classes and DSLs.\nPrefer:\nExplicit conversion over implicit Explicit passing over implicit\nFor our case, if implicits don’t help improve the design of code, we don’t use them. We avoid implicit conversions. We also avoid multi-priority implicits. When type classes help improve the design of code, we use implicits for type classes (IE serialization logic).\nWe prefer implicits to be used for contextual values such as execution contexts or the actor system. However, we prefer to be conservative, and pass the value explicitly for things like validators, authenticators, and normalizers.","title":"On Implicits"},{"location":"/contributing/code-culture.html#on-type-inference","text":"If you are writing some function chain that has multiple levels of inferred function parameter types, we prefer to specify the parameter types.\nPrefer:\ngroups.map { group: Group =>\n  group.apps\n    .groupBy(_.container.image)\n    .map { case (image: String, apps: Seq[App]) =>\n      image -> apps.length\n    }\n  }\nOver:\ngroups.map { group =>\n  group.apps.groupBy(_.container.image).map { case(k, v) =>\n    k -> v.length\n  }\n}","title":"On Type Inference"},{"location":"/contributing/code-culture.html#on-playing-code-golf","text":"Code golf is a game in which you implement some program in as few characters as possible.\nWe prefer not to play this game when working on USI. Instead, we focus on removing noise (boilerplate), while preserving valuable signal pertaining to the problem at hand.","title":"On Playing Code Golf"},{"location":"/contributing/code-culture.html#on-imports","text":"We prefer, almost always, that imports go at the top of the file. Additionally, prefer explicit imports over wildcard imports.\nPrefer:\nimport org.company.{\n  Stringifier,\n  Demuxer\n}\nOver:\nimport org.company._","title":"On Imports"},{"location":"/contributing/code-culture.html#immutability-over-mutability","text":"Functional programming is what follows when you don’t mutate state. We prefer it, except in performance critical parts of our code.\nWe have a strong preference to encapsulate mutable state. References to data crossing the boundary of some module should be immutable.","title":"Immutability Over Mutability"},{"location":"/contributing/code-culture.html#no-smelly-code","text":"We pay attention to code smells and strive to write well encapsulated code.\nA method should not receive more data than it needs to do its job. Changing some behavior should not result in “shotgun surgery”","title":"No Smelly Code"},{"location":"/contributing/code-culture.html#on-todos","text":"TODOs are as liberally used as they are ignored. TODOs should be only used for one of two reasons:\nThings you plan to do before merging Things that are definitely planned in the near future (with a high-priority JIRA)\nInstead of a TODO, prefer:\nCode that fails (e.g. throw a not-implemented exception) A comment describing the pitfalls of the implementation (e.g. this code does not perform very well, or, this code is prone to bugs from possible rounding errors)","title":"On TODOs"},{"location":"/contributing/code-culture.html#fail-loud-and-proud","text":"It is better to do nothing, than to do the wrong thing.\nOne of my favorite non-examples of this is found in PHP 5:\n$ echo '<? print(strftime(\"%Y-%m-%d\",strtotime(\"lol\"))) ?>' | php\n1970-01-01\nAnother non-example is found in Marathon’s history. At one point in time, the storage layer would swallow the exception and return None if it could not deserialize some entity successfully, and this led to data loss.\nWe are strict in what we accept, and in what we emit. If input is not what we expect, don’t be fancy.\nFail loudly. Fail proudly.","title":"Fail Loud and Proud"},{"location":"/contributing/code-culture.html#let-it-crash","text":"We prefer to focus on state recovery, and not graceful tear down. We’d prefer to just crash and restart, rather than implement many complex error recovery handlers.","title":"Let it Crash"},{"location":"/contributing/code-culture.html#dont-panic","text":"We prefer to that Exceptions are thrown only when something goes unexpectedly wrong outside of domain of the library.\nFor example:\nA validation function should return validation errors, not throw them. A storage module will throw an exception if a connection is unavailable.","title":"Don’t Panic"},{"location":"/contributing/code-culture.html#structured-logging","text":"Debugging distributed system is hard. To make this at least a bit easier we favor log messages including logging context (using structured logging). Core USI is providing all necessary tooling for framework developer to log in structured way.\nFor introduction to structured logging read this article.\nAll relevant ids should be part of the logging message e.g.:\npod ID (“podId”) mesos task ID (“taskId”) mesos offer ID (“offerId”)\nIt’s important to keep the naming of keys consistent so that one can use them for log pre-selection.\nEven though structured logging is strongly recommended to every framework developer, including context into the final log message is always optional. We should aim for keeping our messages meaningful even without the context provided (it’s ok to duplicate e.g. podId both in messaage and context).\nExample of good framework log messages (a little bit simplified) can look like example below:\n{\n  \"time\": \"1.1.1970\",\n  \"level\": \"INFO\",\n  \"podId\": \"my-pod\",\n  \"taskId\": \"mesos-task-my-pod-1\",\n  \"message\": \"Task $taskId of Pod $podId was killed\"\n}","title":"Structured logging"},{"location":"/contributing/code-culture.html#testing","text":"Tests are executable documentation and should be written, read and maintained as such. A test tells its reader a short story about how some part of the system should behave given a certain input. Hundreds of excellent books and articles are dedicated to writing proper tests, so we’ll try to avoid repeating them here, however:\nChoose the lightest testing mechanism that suits the need; if something can be tested well at a unit level, then test there first. IE: An integration test might check that a validation error is properly returned via the API, but a unit test should check the 20 different ways a validation fails. Avoid mocks/stubbing. Mocks are often a code smell that happens when effects are coupled with business logic, and the tests often look too much like the implementation. If we separate our effects then we don’t need mocks. Preference for fake implementations of interfaces (IE MockMesos flow) over “method X should be invoked once with param Y and return Z”.","title":"Testing"},{"location":"/contributing/code-culture.html#on-libraries-and-styles","text":"We heavily utilize ScalaTest as our primary test library. All tests should implement the UnitTest class in test-utils, which uses WordSpec.\n// The number of nested levels is dependent on your test case.\n  // For most tests one or two are levels are enough.\n  // Describe a scope for a subject, in this case: \"An empty Set\"\n  \"An empty Set\" should { // All tests within these curly braces are about \"A Set\"\n\n        \"have size 0\" in {    // Here, 'it' refers to \"A Set (when empty)\". The full name\n          Set.empty.size shouldBe 0 // of this test is: \"A Set (when empty) should have size 0\n      }\n      ...\ncombined with Given, When, And Then and scala matchers:\n\"A mutable Set\" should\n\n    \"allow an element to be added\" in {\n      Given(\"an empty mutable Set\")\n      val set = mutable.Set.empty[String]\n\n      When(\"an element is added\")\n      set += \"clarity\"\n\n      Then(\"the Set should have size 1\")\n      set.size shouldBe 1\n\n      And(\"the Set should contain the added element\")\n      set should contain theSameElementsAs Set(\"clarity\")\n    }\n  }\nwhich give us an opportunity to write a test as a short story documenting part of the system behaviour.","title":"On Libraries and Styles"},{"location":"/contributing/code-culture.html#on-testing-granularity","text":"In USI we define following granularity levels:\nUnit tests: the lowest test level possible. Unit test are cheap, can be run fast and give the developer an immediate feedback on when something is obviously broken Integration tests: an integration test starts a minimal Mesos cluster consisting of Mesos master, Mesos agent, in-memory Zookeeper server and a framework. These are more expensive tests that typically require more resources and time to run. Tests that require back and forth communication with Mesos are best placed here because we don’t want to mock Mesos responses. System integraiton test: here we start a full-blown DC/OS cluster testing all aspects of framework interacting with the DC/OS ecosystem. These are the most expensive tests (in terms of time and money) that would typically reqire a DC/OS cluster running on e.g. AWS utilizing multiple EC2 nodes, volumes, ELB etc. A system integration test would typically cover some coarse-grained USI feature or a feature that relies on other DC/OS components like the secret store.\nAs a rule of thumb, a broken behavior should fail at the lowest possible level. Most features will be covered on more than one level but the coverage is different. Let’s consider support for Mesos fetcher as an example. In preparation to run a task, the Mesos fetcher downloads resources into the task’s sandbox directory. So how do the tests look like?\nUnit test: makes sure that given a PodSpec with a defined fetch URI it is converted to a Mesos protobuf message, where correspoding URI fields are initialized properly Integration test: makes sure that when a task from a PodSpec is started, the fetched artifact is actually part of its sandbox System integration test: this is a tricky one; do we need full DC/OS cluster to test this? How is this different from the integration test from the fetchers perspective? The simple answer is that it’s not, and an integration test might be sufficient enough.\nHowever consider the following aspects (which are all taken from prior experience building Mesos frameworks):\nWhile an integration test usually starts some stable Mesos version on which USI officially depends, DC/OS frequently integrates the latest Mesos changes into its master branch. Testing a framework against the latest DC/OS master might expose a bug sooner rather than later. An integration test runs against a local Mesos cluster where communication is typically fast and reliable. A system integration test runs against a cluster somewhere in the cloud, and has a different communications profile. In the past, we’ve seen bugs that would only manifest themselves in the latter case, but not in the former. Admittedly, it seems unlikely that such a bug is triggered in the Mesos fetcher; however, we’ve seen unlikely things happen before. Even a simple request to the framework running on a DC/OS cluster touches many components on its way (e.g. ELB, Admin Router, Monitoring service which themselves rely on other components, such as Admin Router relying on CockroachDB), so there is always potential for things to go wrong.\nIt makes sense to have a system integration test for every user-facing coarse-grained API feature, including the example above.","title":"On Testing Granularity"},{"location":"/design/index.html","text":"","title":"Design"},{"location":"/design/index.html#design","text":"","title":"Design"},{"location":"/design/index.html#intent-of-this-document","text":"This document is intended for developers of the Unified Scheduler Interface (USI) and implementation frameworks that will use it.","title":"Intent of this document"},{"location":"/design/index.html#terminology-","text":"Unified Scheduler Interface (USI): The scheduler component described in this document; an abstraction layer that receives specifications for pods to launch, and handles low level details with Mesos. Implementation Framework: Term used to refer to a framework that uses the USI. Contains orchestration logic and persistence logic pertinent to its logic domain. Scheduler command: Sent by the implementation framework to perform actions; launch pods, kill pods, etc. Pod: A collection of one or more tasks. In the USI (and this document), we deal only with pods. Things formerly known as “apps” or “tasks” are pods with 1 task, with slightly more supported launch options (Docker, etc.). Implementation Framework State: The state of a Framework as best modeled for the Framework’s needs; for example, service specifications, deployment plans, etc. Orchestrator: The orchestrator functionality of a Framework as suited for the Framework’s goals, dealing with the conditional evolution of framework state. RunSpec: Framework managed data structure used to communicate a launch recipe to the USI. Covers resource requirements, networking specification, and a factory to generate a Mesos CommandInfo in order to run one or more tasks. ReserveSpec: A data structure used to communicate with the USI that some reservation to be made, resized, or destroyed. This data structure is provided by the framework.","title":"Terminology:"},{"location":"/design/index.html#overview","text":"In this document we present a design for the USI, in order to support the creation of Frameworks with the following general design goals:\nDeterministic, even in the event of a crash. High scale; can handle workloads with high volume of tasks and deployments. Modular; frameworks can use only what it needs. Stays responsive under large spikes of load. Understandable by mortals (IE framework authors) Help unify integration efforts with Mesos across the variety of Frameworks in the DC/OS ecosystem. Provide a deterministic interface for launching pods.\nIn an effort to achieve these goals, the following concepts will be employed, generally, in the design of USI:\nFast, lock-less serial processing over concurrency. Event sourcing for the replication of state. External-affecting decisions are made in response to the evolution of persisted state, and not in direct response to commands themselves. State transitions and actions are decided upon by pure functions (scheduler has side-effects, decision to make a revive call made by a pure function which receives the current state; will make same decision every time.)\nAt a high-level, there are three major groups of components involved in a framework using USI, and will roughly look like the following:","title":"Overview"},{"location":"/design/index.html#schema","text":"As a rough general overview, schema in USI is expected to look something like this:","title":"Schema"},{"location":"/design/index.html#responsibilities-of-usi","text":"The USI handle as much of the Mesos-specific pod management logic, and reservation logic, without performing any orchestration functions. As an overview, the USI will perform the following jobs:\nFor a given launch command and pod id, launch that pod at-most once. This includes offer matching, constraint evaluation, revive behavior, etc. Provide a mechanism to kill pods, including periodic kill retry Expose the state of those pods in replicable fashion. Reconciles pod statuses with Mesos Cache (persist) and expose non-recoverable facts about pods and reservation state so that those facts are known by future recovering framework processes Manage reservations Expose agent state changes, such as “agent has begun a maintenance window and should drain”. Expose inverse-offer changes\nThe USI will not perform these jobs:\nAutomatically restarting pods Orchestration logic, such as rolling deployment logic Automatically killing unrecognized pods","title":"Responsibilities of USI"},{"location":"/design/index.html#offer-match-statistics","text":"In order to minimize the complexity in the scheduler, the USI will expose a stream of offer match events that can be aggregated by some other component as best serves the needs of the framework. A generic and standard way to do that is left for another design document.","title":"Offer Match Statistics"},{"location":"/design/index.html#storage-of-unrecoverable-facts-from-mesos","text":"There are several pieces of state that cannot be reliably recovered from Mesos under certain unreachable scenarios, or under all scenarios. In order to support deterministic behavior in the framework in the event of a crash / recovery, the USI will store these facts so that the same decisions can be made.","title":"Storage of Unrecoverable Facts from Mesos"},{"location":"/design/index.html#the-fact-that-we-sent-a-task-launch","text":"The USI has launch-at-most-once guarantees. In order to guarantee this in the event of a crash, we have to store the fact that we were about to launch some task on an agent before we actually do so, since, presumably, we could failover and not see the task we launched if the agent in question were unreachable. This needs confirmation, but our understanding is that the Mesos master only stores agent info, and not task info. It should be clarified that we don’t need to store the full offer, or the full response; just the time and the agentId should suffice.\nThe time at which we sent the task launch request is also unrecoverable, and needed as an input for rate-limited launch scenarios.","title":"The fact that we sent a task launch"},{"location":"/design/index.html#certain-taskstatus-attributes","text":"Mesos does not retain the first time a task was unreachable, or the first time a Mesos health check was reported unhealthy.","title":"Certain TaskStatus attributes"},{"location":"/design/index.html#agent-info","text":"Lamentably, the agent info can be forgotten by Mesos when it is unreachable for more than agent_ping_timeout * max_agent_ping_timeouts. Agent info is needed as an input for certain types of offer-matching constraints, such as the Marathon MAX_PER constraint or UNIQUE constraint.\nIf the Agent info is forgotten, the pods may still come back later (source: I tested this myself).","title":"Agent info"},{"location":"/design/index.html#certain-facts-about-reservations","text":"We’ll store certain things about reservations, such as “reservation last seen”.","title":"Certain facts about reservations"},{"location":"/design/index.html#unrecognized-reservations-and-unrecognized-pods","text":"A unrecognized pod and unrecognized reservation is defined as a pod or reservation belonging to the implementation framework for which there is no persistent record (PodRecord or ReservationRecord) of it being launched.\nIn the case of a unrecognized pod being discovered, the USI will expose a PodStatus for the unrecognized pod to the implementation framework.\nSimilarly, unrecognized pods pertaining to the framework may surface. Rather than automatically kill these pods, the scheduler component will expose their status as best as it can, so that the framework can best decide what to do with them (PodStatus without a PodRecord).\nUnfortunately, Mesos does not expose task grouping information, but instead reports individual task statuses. As such, unrecognized tasks will be reported as PodStatuses with a task count of 1, each.not expose this information in TaskGroup.","title":"Unrecognized reservations and unrecognized pods"},{"location":"/design/index.html#constraints-application","text":"The scheduler component itself will provide an unopinionated, pluggable approach to the application of constraints. The USI will provide a library of predefined constraints available for use that the framework may optionally choose to use. Implementation frameworks will provide constraints to the USI via the RunSpec. The language for exposing these constraints to the end-user is left to the implementation framework.","title":"Constraints Application"},{"location":"/design/index.html#storage-and-message-ordering-guarantees","text":"In order to support deterministic behavior in implementation frameworks, Message ordering will be preserved. Were it not so, we could do things such as send Mesos task statuses out of order.\nSome messages require data to be persisted before we can act on them. All messages, whether they affect the persisted state or not, will be threaded through the scheduler persistence layer.\nThis does not imply that we store offers.","title":"Storage and Message Ordering Guarantees"},{"location":"/design/index.html#decoupling-scheduler-input-from-implementation-framework-state","text":"An implementation framework launches pods and creates reservations by specifying RunSpecs and ReserveSpecs. However, depending on the implementation framework, different aspects of this runSpec come from the user. For example, a user may specify memory requirements, but not specify the launch command or container image (this being managed by the implementation framework deploying a deta service of some kind).\nTherefore, by design USI does not persist the runSpec. It only persists the record of launching pods or reservations.","title":"Decoupling Scheduler Input from Implementation Framework State"},{"location":"/design/index.html#example-interaction-diagram","text":"Implementation frameworks will be responsible for storing and maintaining what needs to be launched, and for handling orchestration logic; the scheduler solely handles the launching of pods (with at-least-once guarantees), and providing guarantees about which aspects of these pod states the implementation framework can get back. This has the following benefits:\nFlexible framework schema: Implementation frameworks can store the information about tasks to launch and schedules as it suits the framework; it is not necessary for the USI to support every different orchestration concern under the sun Incremental adoption path: Scheduler component can be integrated in to any existing framework that can emit the appropriate launch commands with runSpecs; statuses emitted by scheduler can be mapped back to entities that the implementation framework understands. Generally solve the problem of deterministic orchestration: Implementation frameworks can rely on podRecords to ensure that an instance of some service isn’t doubly launched in the event of failover.\nThe following interaction diagram should help illustrate roughly how the scheduler component will interact with other layers.","title":"Example Interaction Diagram"},{"location":"/design/index.html#questions","text":"","title":"Questions"},{"location":"/design/index.html#how-does-the-scheduler-notify-the-orchestrator-that-an-agent-is-going-away-","text":"AgentInfo will be exposed via the podStatus. If the agent’s information is detected to be updated, an PodStatus update will be emitted for all pods pertaining to the agent with the new agent information. The orchestrator can react to this new state and begin transitioning pods off the agent in a controlled fashion.","title":"How does the scheduler notify the orchestrator that an agent is going away?"},{"location":"/design/index.html#what-about-pod-relaunch-rate-limiting-","text":"This is an area that needs further consideration and design. To keep the scheduler component simple and concerns separated, ideally the scheduler knows nothing about backoff delay, and another component rate-limits launch commands in-transit according to the backoff mechanism.\nWe may also consider if configurable per-service rate limiters are even necessary, and potentially just throttle by pod ids globally.","title":"What about pod relaunch rate-limiting?"},{"location":"/design/index.html#what-about-logging-","text":"It’s anticipated that logging will be handled through the generic slf4j interface. Implementation frameworks will be responsible to provide implementations of their choosing.","title":"What about logging?"},{"location":"/design/index.html#what-about-metrics-tracing-","text":"This is an area that needs design, but can be done at a later date and does not affect the overall architecture of the USI.","title":"What about metrics / tracing?"},{"location":"/design/index.html#benchmarking","text":"The USI will have a simple framework test harness used to benchmark its performance, independently, to avoid performance regressions.","title":"Benchmarking"},{"location":"/design/index.html#how-will-java-consume-this-","text":"The inputs and outputs will be exposed as reactive stream Publishers and Subscribers. Reactive streams has been merged in to the standard library as of Java 9 and is available as a polyfill dependency for Java 8.\nA Java interface will be provided for generating related data structures, interacting with constraints, reading PodStatus / ReservationStatus, etc.","title":"How will Java consume this?"},{"location":"/design/index.html#how-will-we-react-to-spikes-in-offers-received","text":"The USI will queue a configurable number of offers. If this queue is exceeded, offers without reservations pertaining to the framework will be “quick declined”, without evaluation.","title":"How will we react to spikes in offers received"},{"location":"/design/index.html#how-will-reservations-be-labeled-associated-with-the-framework-","text":"Reservations for single role frameworks will be handled with labels. If needed, we will provide an pluggable strategy for detecting reservations belonging to us, and specifying them.\nFor multi-role reservations, we can explore using a framework-specific role (the framework ID?). This could in theory enable more efficient revives when trying to re-launch resident pods.","title":"How will reservations be labeled / associated with the Framework?"},{"location":"/design/index.html#how-will-offer-revive-be-handled-","text":"Offer revive will be automatically managed by the USI, as needed. However, there could be different revive policies needed (more aggressive, less aggressive, due to the framework?). Some investigation and further design is warranted here.","title":"How will offer revive be handled?"},{"location":"/design/index.html#how-do-you-restart-a-pod-","text":"A pod is restarted by sending a Kill command for some pod id. Then, once USI reports said pod as terminal, send a command to expunge the podRecord, and send a command to launch a new pod.","title":"How do you restart a pod?"},{"location":"/design/index.html#what-constraints-are-there-on-pod-task-ids-","text":"The USI, out of necessity, allows any valid task ID as a pod / task ID. Were it not so, frameworks could not move to the USI without killing all existing pods and reservations (since said IDs are immutable).\nAn option we can explore is providing modeled IDs to the USI, with one of the variants of the modeled ID being “freeform” (no associated metadata implied). Benefits of this approach could allow the USI to:","title":"What constraints are there on Pod / task IDs?"},{"location":"/design/index.html#how-does-the-usi-report-schema-violation-errors-to-the-framework","text":"The Framework is responsible for sanitizing user input. Any failures to provide the USI valid input will be considered a bug.\nIf it does happen, one mechanism for reporting errors could be to output an erroneous podStatuses with the validation errors. This is an area open for further design.","title":"How does the USI report schema violation errors to the framework"},{"location":"/design/index.html#how-does-a-framework-resize-reservations-","text":"Reservations will be resized by specifying a new reservation spec for the same reservation ID. The status for the reservation will be exposed via the ReservationStatus on the other end, “resizing”, “resize failed”, etc.","title":"How does a Framework resize reservations?"},{"location":"/design/index.html#which-version-of-the-mesos-client-will-the-usi-use-","text":"The USI will use the Mesos v1 client","title":"Which version of the Mesos client will the USI use?"},{"location":"/design/index.html#is-leader-election-a-concern-of-the-unified-scheduler-","text":"No.","title":"Is leader election a concern of the unified scheduler?"},{"location":"/design/index.html#what-if-the-framework-wants-to-use-a-different-data-store-other-than-zookeeper-","text":"Initially, the USI will be opinionated towards storing its state in Zookeeper. Theoretically, any persistence mechanism that can preserve the order of writes would work as well. A pluggable storage module could be introduced at a future point.","title":"What if the framework wants to use a different data store, other than Zookeeper?"},{"location":"/design/index.html#why-not-just-auto-kill-something-for-which-a-podrecord-no-longer-exists-","text":"This is the behavior that Marathon has historically had, and has led to upsetting problems when the persistent state comes invalid or out-of-date (for example, a backup is restored because an upgrade failed). Preferably, how to handle unrecognized pods and reservations is up to the implementation framework.","title":"Why not just auto-kill something for which a podRecord no longer exists?"},{"location":"/design/index.html#relevant-resources","text":"CQRS Faq Scheduler Loop Design Slides Martin Fowler: The LMAX Architecture Lock Fee Algorithms for Ultimate Performance Slides Reactive Manifesto","title":"Relevant Resources"},{"location":"/examples/index.html","text":"","title":"Examples"},{"location":"/examples/index.html#examples","text":"","title":"Examples"},{"location":"/examples/keep-alive-framework.html","text":"","title":"Keep-Alive Example Framework"},{"location":"/examples/keep-alive-framework.html#keep-alive-example-framework","text":"Run the keep-alive example framework that: - uses simplified Scheduler interface - starts configurable amount of echo \"Hello, world\" && sleep 20 tasks (default 100) - keeps restarting the task if it finishes","title":"Keep-Alive Example Framework"},{"location":"/examples/keep-alive-framework.html#deployment-on-dc-os-enterprise","text":"Launch strict cluster and setup the DC/OS CLI. Create key pair: dcos security org service-accounts keypair usi.private.pem usi.pub.pem Create user usi: dcos security org service-accounts create -p usi.pub.pem -d \"For testing USI on strict\" usi Store private key as secret: dcos security secrets create -f ./usi.private.pem usi/private_key Grant usi access: dcos security org users grant usi dcos:mesos:master:task:user:nobody create dcos security org users grant usi dcos:mesos:master:framework:role:usi read dcos security org users grant usi dcos:mesos:master:framework:role:usi create Deploy the framework: dcos marathon app add keep-alive-framework-app.json","title":"Deployment on DC/OS Enterprise"}]}